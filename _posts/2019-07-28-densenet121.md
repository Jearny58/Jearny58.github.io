---
layout: post
published: false
title: DenseNet121
---
This post serves as notes gathered on the DenseNet deep learning algorithm from July 30, 2019. The notes are from mutliple articles and links to the sources will be provided for each one. What I am hoping to do with this post (and subsequent posts on neural networks in general, convolutional neural networks, ResNet, etc.) is to not only get a better understanding of what is going on underneath the hood, but also to curate in one spot material that'll help with the comprehensive report for my second capstone project. 

Without further ado, let's get started!

## [A Comprehensive Guide to Convolutional Neural Networks - the ELI4 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)
__[by Sumit Saha](https://towardsdatascience.com/@_sumitsaha_)__
- monumnetal growth, especially in domain of computer vision (i.e. image classification)
	- computer vision agenda: enable machines to view world as humans do
    - advancements primarily due to convolutional neural network algorithm

__Introduction__
- ConvNet/CNN = Convolutional Neural Network
	- takes input image, assigns importance (through weights/bias) to aspects/objects in image
    	- hopefully helps algorithm to differentiate one from the other
    - features do not need to be hand-engineered
    - with enough training, have ability to learn characteristics

__Why ConvNets over Feed-Forward Neural Nets?__
- image = matrix of pixel values
- multi-level perceptron not able to capture nuances of complex images (i.e. pixel dependencies)
- ConvNet = better able to capture spatial and temporal dependencies
	- performs better due to reduction in parameters, reusability of weights

__Input Image__
- varying color spaces for images, few examples below
	- grayscale: varying ranges of intensity, forming a gradient from completely white to completely black
    - RGB: red, green and blu light added together in various ways to reproduce a broad array of colors [(Wikipedia)](https://en.wikipedia.org/wiki/RGB_color_model)
    - HSV: hue, saturation, value; alternative representation of RGB, made to more closely align with way human vision perceives color-making attributes [(Wikipedia)](https://en.wikipedia.org/wiki/HSL_and_HSV)
    - CMYK: subtractive color model, used in color printing and refers to the four ink plates used in some color printing - cyan, magenta, yellow, key (black) [(Wikipedia)](https://en.wikipedia.org/wiki/CMYK_color_model)
- Once images reach high dimensions, computation starts to escalate
	- where ConvNet comes into play, reduces image into form that is easier to process
    	- without losing critical features
        
__Convolution Layer -- The Kernel__


![cnn_kernel.gif]({{site.baseurl}}/img/cnn_kernel.gif)
_Image can found [here](https://images.app.goo.gl/HswakXMjdsMwWuAU6)_

- yellow box represents the kernel
	- in our case it is a 3x3x1 matrix
- kernel shifts 9 times, each time performing matrix multiplication between the kernel and that particular portion of the image
	- the shifting of the kernal has to do with something called stride length
    	- stride length can be changed, impacts how far kernel 'shifts' over in image space


![cnn_kernel.gif]({{site.baseurl}}/img/cnn_1.gif)
_Image found [here](https://www.researchgate.net/post/How_will_channels_RGB_effect_convolutional_neural_network)_

- above is an example of multiple channel image (i.e. RGB)
	- sums output of matrix multiplication for each channel with bias
    - this gives us squashed one-depth channel Convoluted feature output
- objective of convolution operation: extract high-level features such as edges from input image
	- don't need to be limited to only on convolutional layer
    - first layer responsible for low-level features
    - with added layers, architecture adapts to higher-level features (see example below)

![eagle]({{site.baseurl}}/img/eagle_dl.jpeg)
_Image and following notes were found on Quora at this [link](https://www.quora.com/What-is-the-difference-between-high-level-features-and-low-level-features)_

- What the ConvNet is learning by layer
	- first layer might detect edges of image
    - second layer detects curves of the bird, lines of the waves
    - even higher levels start detecting things like the entire bird
    


